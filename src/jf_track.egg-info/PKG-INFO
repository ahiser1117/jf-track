Metadata-Version: 2.4
Name: jf-track
Version: 0.1.0
Summary: Add your description here
Requires-Python: >=3.12
Description-Content-Type: text/markdown
Requires-Dist: cfgrib>=0.9.15.1
Requires-Dist: h5netcdf>=1.7.3
Requires-Dist: ipykernel>=7.1.0
Requires-Dist: ipympl>=0.9.8
Requires-Dist: mat4py>=0.6.0
Requires-Dist: matplotlib>=3.10.7
Requires-Dist: nbformat>=5.10.4
Requires-Dist: netcdf4>=1.7.3
Requires-Dist: numpy>=2.3.5
Requires-Dist: opencv-python>=4.11.0.86
Requires-Dist: pandas>=2.3.3
Requires-Dist: plotly>=6.5.0
Requires-Dist: rioxarray>=0.20.0
Requires-Dist: scikit-image>=0.25.2
Requires-Dist: scikit-learn>=1.7.2
Requires-Dist: scipy>=1.16.3
Requires-Dist: seaborn>=0.13.2
Requires-Dist: tables>=3.10.2
Requires-Dist: xarray>=2025.11.0
Requires-Dist: zarr>=3.1.5
Requires-Dist: fastapi>=0.115.6
Requires-Dist: uvicorn>=0.32.1

# jf-track

A video tracking system for jellyfish microscopy, featuring two-pass detection of anatomical features and adaptive background subtraction for rotating backgrounds.

NOTE: This app was entirely developed using Agent-based development tools (Claude Code, Codex, OpenCode).

## Features

- **Two-pass tracking**: Detects larger features (mouth) and smaller features (bulbs) separately with different size thresholds
- **Episode-based median background**: Computes a median background from evenly spaced frames within each static episode
- **Rotation-compensated background**: Handles rotating samples with ORB + RANSAC and a STATIC/ROTATING/TRANSITION state machine
- **Direction analysis**: Computes orientation vector from bulb center-of-mass to mouth position
- **Circular ROI masking**: Constrains tracking to central region where jellyfish is expected
- **Search-radius filtering**: Optional spatial filters for mouth re-acquisition and bulb detection

## Installation

### Using uv (recommended)

```bash
# Clone the repository
git clone https://github.com/ahiser1117/jf-track.git
cd jf-track

# Install dependencies (creates .venv automatically)
uv sync
```

### Using pip

```bash
# Clone the repository
git clone https://github.com/ahiser1117/jf-track.git
cd jf-track

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install the package and its dependencies
pip install .
```

### Dependencies

- Python 3.12+
- Core tracking dependencies: OpenCV (`opencv-python`), NumPy, SciPy, scikit-image, zarr
- See `pyproject.toml` for the full dependency list (analysis notebooks and optional tooling)

## Quick Start

```python
from src.tracking import run_two_pass_tracking, merge_mouth_tracks
from src.direction_analysis import compute_direction_analysis
from src.save_results import save_two_pass_tracking_to_zarr
from src.visualizations import save_two_pass_labeled_video

# Run tracking
mouth_tracking_raw, bulb_tracking, fps, params = run_two_pass_tracking(
    "path/to/video.avi",
    max_frames=1000,
    background_buffer_size=10,
    adaptive_background=True,  # Enable if background rotates
)

# Merge mouth track segments (handles occlusion gaps)
mouth_tracking = merge_mouth_tracks(mouth_tracking_raw)

# Compute direction analysis
direction = compute_direction_analysis(mouth_tracking, bulb_tracking)

# Save results
save_two_pass_tracking_to_zarr(
    mouth_tracking,
    bulb_tracking,
    direction,
    "output.zarr",
    fps,
    parameters=params,
)

# Create visualization
save_two_pass_labeled_video("path/to/video.avi", "output.zarr", "labeled.mp4")
```

## How It Works

### Background Subtraction

The system uses **episode-based median backgrounds** computed from evenly spaced frames within each static episode (default 10 samples). This adapts to gradual illumination changes while keeping a stable reference for subtraction.

When `adaptive_background=False`, the entire video is treated as a single static episode.

For videos where the sample rotates:
1. Frame-to-frame rotation is estimated using ORB feature matching
2. When rotation exceeds a threshold, the system enters "rotating" mode
3. Buffered frames are rotated to align with the current orientation before computing the median
4. After rotation stops, the system transitions back to static mode and builds a new static background

Tracking is **skipped during ROTATING/TRANSITION** states; detections only occur during STATIC episodes.

### Two-Pass Detection

Objects are detected using connected component analysis on the background-subtracted binary mask:

1. **Mouth detection**: Finds objects with area between `mouth_min_area` and `mouth_max_area` (default 35-160 pixels)
2. **Bulb detection**: Finds smaller objects between `bulb_min_area` and `bulb_max_area` (default 5-35 pixels)

Both passes use the same background-subtracted mask but with different size filters.

### ROI Masking

A circular region-of-interest mask constrains detection to the center of the frame:
- Center: `(width/2, height/2)`
- Radius: `min(width, height)/2`

Objects outside this region are ignored.

### Track Merging

The mouth may be temporarily lost due to occlusion. The `merge_mouth_tracks()` function links non-overlapping track segments into a single continuous track. When tracks overlap (rare), the track closest to the last known position is preferred; if no prior position exists, the larger-area detection is used.

### Direction Analysis

For each frame, the system computes:
- Bulb center-of-mass (average position of all detected bulbs)
- Direction vector from bulb CoM to mouth
- Direction angle and magnitude (0° = right, 90° = up in image coordinates)

Positions are temporally smoothed to reduce noise.

## Output Format

Results are stored in zarr format with the following structure:

```
output.zarr/
├── mouth/
│   ├── track     # Track IDs
│   ├── x, y      # Positions (n_tracks, n_frames)
│   ├── area      # Detection area
│   └── ...
├── bulb/
│   └── ...       # Same structure as mouth
└── direction/
    ├── mouth_x, mouth_y
    ├── bulb_com_x, bulb_com_y
    ├── bulb_count
    ├── direction_x, direction_y
    ├── direction_magnitude
    └── direction_angle_deg
```

Tracking parameters are stored in zarr attributes (`parameters`) when provided to `save_two_pass_tracking_to_zarr()`.

## Visualization

The `save_two_pass_labeled_video()` function creates annotated videos with:
- Orange circles for mouth position
- Cyan circles for bulb positions
- Yellow crosshair for bulb center-of-mass
- Green arrow showing direction vector
- Gray circle showing ROI boundary

Background modes:
- `"original"`: Original video frames
- `"diff"`: Background subtraction difference (shows what tracking sees)
- `"mask"`: Binary mask after thresholding

`save_two_pass_labeled_video()` can auto-load tracking parameters from the zarr store to keep visualization aligned with the tracking run (thresholds, buffer size, search radii, adaptive background settings).

## Configuration

Key parameters for `run_two_pass_tracking()`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `background_buffer_size` | 10 | Frames sampled per static episode for median background |
| `threshold` | 10 | Binary threshold for background subtraction |
| `mouth_min_area` | 35 | Minimum mouth area (pixels) |
| `mouth_max_area` | 160 | Maximum mouth area (pixels) |
| `bulb_min_area` | 5 | Minimum bulb area (pixels) |
| `bulb_max_area` | 35 | Maximum bulb area (pixels) |
| `adaptive_background` | False | Enable rotation compensation |
| `rotation_start_threshold_deg` | 0.01 | Rotation detection threshold (deg/frame) |
| `rotation_stop_threshold_deg` | 0.005 | Rotation stop threshold (deg/frame) |
| `mouth_search_radius` | None | Max distance from last mouth position for re-detection |
| `bulb_search_radius` | None | Max distance from mouth for bulb filtering |

## License

MIT License
